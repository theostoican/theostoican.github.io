- intro to dimensionality reduction
- intuition
- 

# Introduction

In machine learning, working in high-dimensional spaces is often either computationally intractable or does not yield good practical results. Firstly, the curse of dimensionality (which helds that the more features/dimensions a data sample has, the more samples we need in order to accurately model the distribution of the data) compels us to find more data, thus making training and inference prohibitively expensive. Secondly, it is often the case that information in real-world data is embedded in only a few dimensions. Having more dimensions raises modelling issues by complicating the problem and leading to a lack of generalization. Thus, one may often want to perform *dimensionality reduction* before modelling, in order to capture the most essential features of the input data. Enter PCA.

# PCA - intuition

PCA is an algorithm that tries to identify the most import dimensions of the input data (principal components) and remove the unnecessary ones. But how does PCA identify the unnecessary dimensions? For that, let us have a look at the following image.


<div style="text-align: center;">
{% include figure.html path="assets/img/pca/pca_2_dim_data.png" class="img-fluid rounded z-depth-1" %}
Â© https://web.stanford.edu/~jurafsky/slp3/
</div>