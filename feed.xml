<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://theostoican.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://theostoican.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-29T15:03:51+00:00</updated><id>https://theostoican.github.io/feed.xml</id><title type="html">blank</title><subtitle>A blog for ML-dedicated material and personal thoughts. Disclaimer: Opinions are my own.
</subtitle><entry><title type="html">PCA</title><link href="https://theostoican.github.io/blog/2023/pca/" rel="alternate" type="text/html" title="PCA" /><published>2023-04-06T00:00:00+00:00</published><updated>2023-04-06T00:00:00+00:00</updated><id>https://theostoican.github.io/blog/2023/pca</id><content type="html" xml:base="https://theostoican.github.io/blog/2023/pca/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>In machine learning, working in high-dimensional spaces is often either computationally intractable or does not yield good practical results. Firstly, the curse of dimensionality (which helds that the more features/dimensions a data sample has, the more samples we need in order to accurately model the distribution of the data) compels us to find more data, thus making training and inference prohibitively expensive. Secondly, it is often the case that information in real-world data is embedded in only a few dimensions. Having more dimensions raises modeling issues by complicating the problem and leading to a lack of generalization. Thus, one may often want to perform <em>dimensionality reduction</em> before modeling, in order to capture the most essential features of the input data. Enter PCA.</p>

<h1 id="pca---intuition">PCA - intuition</h1>

<p>PCA is an algorithm that tries to identify the most import dimensions of the input data (principal components) and remove the unnecessary ones. But how does PCA identify the unnecessary dimensions? For that, let us have a look at the following image.</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pca/pca_2_dim_data-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pca/pca_2_dim_data-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pca/pca_2_dim_data-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pca/pca_2_dim_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

An example of two-dimensional data that is negatively correlated (whenever x increases, y decreases) in its raw form.
</div>

<p>Were we to look at the data from the perspective of the 2 coordinates individually, we would be tempted to assume that each corresponding dimension contains a substantial amount of data and is indispensable. However, were we to choose a
different system of coordinates, we could see a disproportion insofar as the quantity of information might not as dispersed as we would think it is.</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pca/pca_2_dim_data_new_coord-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pca/pca_2_dim_data_new_coord-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pca/pca_2_dim_data_new_coord-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pca/pca_2_dim_data_new_coord.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

The same data as above, but represented using a different system of coordinates.
</div>

<p>If we look at the figure from above, we see that, across one of the axes the information is not as widely spread as in the first system of coordinates. What this means in practice is that we may want to get rid of this dimension for the sake of preserving the most relevant information for modeling and so perform some dimensionality reduction on the input data. Seemingly we have achieved our goal of reducing the dimensionality of the data, even though the data was initially not obviusly reducible. The question, however, of finding the appropiate system of coordinates in which one could perform dimensionality reduction remains open.</p>

<h1 id="pca---in-detail">PCA - In Detail</h1>

<p>Let us delve into the more intricate details of PCA in order to see how one could find the appropiate system of coordinates.</p>

<h2 id="reducing-the-covariance-of-the-data">Reducing the Covariance of the Data</h2>

<p>The tenet of finding such a system of coordinates is the covariance between the dimension of the input data. In the last image from above, we see that the input dimensions have a correlation of 0. Therefore, we can examine the spread of each of the dimensions individually and decide which ones do not contain critical information for modeling and thus can be removed. Ideally, we want to find such a decorrelated system of coordinates for any input data that we have. Fortunately, there is a theoretically sound way of finding it. But, to begin with, let us first look at the covariance of the input data.</p>

<p>Let us assume, for the sake of generality, that we have a d-dimensional input data.</p>

\[X = \left[\begin{array}{ccc}
x_{11} &amp; \ldots &amp; x_{1d}\\
x_{21} &amp; \ldots &amp; x_{2d}\\
\vdots &amp; \ddots &amp; \vdots\\
x_{N1} &amp; \ldots &amp; x_{Nd}
\end{array}\right]\]

<p>The covariance matrix of \(X\) would be a \(D x D\) matrix of the form:</p>

\[\Sigma_{X} =  \left[\begin{array}{ccc}
Var(X_1) &amp; \ldots &amp; Cov(X_1, X_d)\\
Cov(X_2, X_1) &amp; \ldots &amp; Cov(X_2, X_d)\\
\vdots &amp; \ddots &amp; \vdots\\
Cov(X_d, x_1) &amp; \ldots &amp; Var(X_d)
\end{array}\right]\]

<p>As an observartion, this is a square and symmetric matrix. We know that real symmetric matrices are diagonizable and always admit a spectral decomposition. Hence, decomposing our matrix \(\Sigma_{X}\) leads to:</p>

\[\Sigma_{X} = \Gamma \cdot \Lambda \cdot \Gamma^T\]

<p>In the decomposition, \(\Lambda\) is a diagonal matrix, that has 0 as values for all elements besides the ones that reside on the diagonal. But this is exactly what we wished for before! A covariance matrix, where all the covariances between every two distinct dimensions are 0. Now, the question that remains is how do we transform our data such that it has this covariance?</p>

<h2 id="transforming-the-input">Transforming the Input</h2>

<p>We know that eigendecomposition finds a new vector space (where the eigenvectors form an orthonormal basis). One way would be to project the vectors in our original data matrix onto the newly found vector space. For that, multiplying with \(\Gamma\) should suffice. Therefore, our new data would be:</p>

\[Y = X \cdot \Gamma\]

<p>The new data would have the covariance as found above: \(\Lambda\). To make sure, let us redo the calculation. For simplification and without loss of generalization, let us consider a \(2 x 2\) input matrix:</p>

\[X = \left[\begin{array}{cc}
X_{11} &amp; X_{12}\\
X_{21} &amp; X_{22}
\end{array}\right]\]

<p>This can be rewritten as:</p>

\[X = \left[\begin{array}{c}
X^1\\
X^2
\end{array}\right]\]

<p>After transformation:</p>

\[Y = X \cdot w =  \left[\begin{array}{cc}
X^1w_1 &amp; X^1w_2 \\
X^2w_1 &amp; X^2w_2
\end{array}\right] = \left[\begin{array}{cc}
Xw_1 &amp; Xw_2 
\end{array}\right]\]

<p>Then, the covariance matrix of the transformed input would be:</p>

\[\Sigma_{Y} = \left[\begin{array}{cc}
Var(Y_1) &amp; Cov(Y_1, Y_2) \\
Cov(Y_2, Y_1) &amp; Var(Y_2)
\end{array}\right]\]

<p>We would expect at this point that \(Cov(Y_1, Y_2) = 0\) and \(Cov(Y_2, Y_1) = 0\). For the sake of keeping things short enough, we will only see that \(Cov(Y_1, Y_2) = 0\). The demonstration is similar for \(Cov(Y_2, Y_1)\).</p>

\[Cov(Y_1, Y_2) = \mathbb{E}[Y_1Y_2] - \mathbb{E}[Y_1]\mathbb{E}[Y_2]\]

\[Cov(Y_1, Y_2) = \frac{X^1w_1X^1w_2 + X^2w_1X^2w_2}{2} - \frac{X^1w_1 + X^2w_1}{2} \cdot \frac{X^1w_2 +X^2w_2}{2}\]

<p>We know that the dot product is associative with respect to a scalar: \(c(a \cdot b) = a \cdot (cb)\). We can use this in our case as well, since \(X^1w_1\) is a scalar and thus \(X^1w_1(X^1w_2) = X^1(X^1w_1w_2)\). But \(w_1w_2 = 0\), due to the orthonormality property of the eigenvectors. Thus, the whole thing is \(0\). More specificially:</p>

\[Cov(Y_1, Y_2) = \frac{X^1(X^1w_1w_2) + X^2(X^2w_1w_2)}{2} - \frac{X^1 + X^2}{2} \cdot w_1 \cdot \frac{X^1 +X^2}{2} \cdot w_2\]

<p>Both terms are 0 for the reason mentioned above and thus:</p>

\[Cov(Y_1, Y_2) = 0\]

<p>Similarly one can show that \(Cov(Y_2, Y_1) = 0\) and thus the covariance between every two dimensions of the projected input is 0.</p>

<h3 id="reducing-the-input-dimensionality">Reducing the Input Dimensionality</h3>

<p>So far, we have shown that we can find a new system of coordinates (algebraically, we can perform a change of basis) such that the covariance between every two dimensions is 0. Now, what we ought to do is to choose which dimensions we are going to keep and which we are going to drop. As a rule of thumb, one could look at the variance across each dimension in the \(\Lambda\) matrix (which corresponds to an eigenvalue) and decide which dimensions have a small variance. If the variance is reasonably small (up to be decided by the person who does the modelling) then we can give up on that dimension. This has repercussions on the space on which we project the input though. If we project on a lower-dimensional space, then one has to drop an eigenvalue and an eigenvector. Therefore, instead of multiplying by \(\Gamma\), one has to reduce the columns from \(\Gamma\) that correspond to the smallest eigenvalues, obtain a new \(\Gamma_{trunc}\) and perform the projection in the same way, by doing \(X \cdot \Gamma_{trunc}\).</p>

<p>In this way, one can perform dimensionality reduction by reducing the least informative dimensions.</p>]]></content><author><name></name></author><category term="pca" /><category term="basic-ml" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Naïve Bayes</title><link href="https://theostoican.github.io/blog/2023/naive-bayes/" rel="alternate" type="text/html" title="Naïve Bayes" /><published>2023-03-26T00:00:00+00:00</published><updated>2023-03-26T00:00:00+00:00</updated><id>https://theostoican.github.io/blog/2023/naive-bayes</id><content type="html" xml:base="https://theostoican.github.io/blog/2023/naive-bayes/"><![CDATA[<h1 id="requirements">Requirements</h1>
<ul>
  <li>Deep understanding of Bayes’ rule</li>
  <li>Machine Learning fundamentals (Maximum Likelihood Estimation, what inference, training mean)</li>
</ul>

<h1 id="problem">Problem</h1>

<p>One of the common tasks in natural language processing is the classification of some specific text. We may want to see if it’s spam or anti-spam, if it reflects positive or negative sentiments, if the author is male or female, and so forth. Before the actual classification task though, some processing on the text is required in order to ease the classification task (we cannot just feed the data as-is into the model). We will kick off with this task in the first paragraph and then we will extend the context with the actual classification reasoning.</p>

<h1 id="assumptions">Assumptions</h1>

<h3 id="caveat">Caveat</h3>

<p>In NLP, we commonly refer to a <em>text</em> (which can contain a review, a novel, an email) as a <em>document</em>. A collection of documents forms a dataset.</p>

<h2 id="bag-of-words">Bag-of-words</h2>

<p>In order to use a Machine Learning model, one should obtain some features based on the respective text. What kind of features can one get? One intuitive way is to assume as features the frequency of the words present in the text. If word ‘fabulous’ is used 2 times in the text, one could think that the text is positive (unless ‘fabulous’ is used sarcastically). Hence, we can collect the numbers of occurrences of each word in the text and use them as features for a document. Therefore, by using this approach, we are discarding the sequential relationship between words in the text. This is commonly called the <strong>bag-of-words</strong> assumption - we are essentially discarding the sequential information and treat the text like a “bag” of randomly positioned words.</p>

<!-- <figure>
    <img src="/img/bagofwords.png" alt='missing' />
    <figcaption align="center">© https://web.stanford.edu/~jurafsky/slp3/</figcaption>
</figure> -->

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bagofwords-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bagofwords-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bagofwords-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bagofwords.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

© https://web.stanford.edu/~jurafsky/slp3/
</div>

<h1 id="model">Model</h1>

<p>The next thing one should consider is what kind of modeling we can do now. Remember, our main goal is to classify this text (we’ll keep it rather generic in this text and we consider classifications of any type - although, you can think of sentiment analysis as a particular example - we want to predict what sentiment the author had when writing the specific text/review/tweet/etc.). Intuitively, one way to model this problem and to put it into a Machine Learning framework is to assign a probability distribution to the class, given the text:</p>

\[P(positive|text) = some\_prob\]

\[P(negative|text) = 1 - some\_prob\]

<p>In this framework, we can make predictions about the document (is it positive or negative and how accurate our prediction is ?). Furthermore, the document is just a collection of word features, as we said in the <em>bag-of-words</em> assumption. More specifically, the probabilities from above can be decomposed as follows:</p>

\[P(positive | text) = P(positive | word\_freq_1 , word\_freq_2, ...)\]

<p>Now, given this probability distribution, how can we compute it?</p>

<h2 id="generative-and-discriminative-models">Generative and discriminative models</h2>

<p>In Machine Learning, there are 2 categories of models that are used for classification tasks:</p>

<ul>
  <li>generative (models learn the distribution that the data belongs to and can generate data)</li>
  <li>discriminative (models learn a function based on the existent features in order to <em>discriminate</em> between the existing classes)</li>
</ul>

<p>As usual, there is a trade-off between these 2 types of models.</p>

<ul>
  <li>Generative models have the advantage of being able to generate data which is useful in various scenarios. The drawback is that they often impose a constraint on the data (the data is Gaussian distributed, or, more generally, distributed according to a predefined distribution). It is for this reason that generative models don’t have too much flexibility when learning a distribution.</li>
  <li>Discriminative models, on the other hand, can be particularly useful for figuring out the underlying mathematical function that maps the features to a specific output (in this case, the class). The drawback is, naturally, the fact that this models cannot generate data, since they just learn a mathematical function, which always needs an input in order to discriminate.</li>
</ul>

<p>In a nutshell:</p>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">Generative Models</th>
      <th style="text-align: right">Discriminative Models</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Flexibility</td>
      <td style="text-align: center">:x:</td>
      <td style="text-align: right">:white_check_mark:</td>
    </tr>
    <tr>
      <td style="text-align: left">Data generation</td>
      <td style="text-align: center">:white_check_mark:</td>
      <td style="text-align: right">:x:</td>
    </tr>
  </tbody>
</table>

<p><br />
Now, for the sake of this problem, we assume that we want to make use of a generative model. Hence, according to the assumptions made by generative models in general, we will characterize our data by providing distributions for each class \(P(c)\) and distributions for \(P(f_i | c)\), where \(f_i\) - one of the features that we’re using (as we saw before, we’re considering as features the occurrences of each of the words within the text - \(f_i=word\_freq_i\)). So, given this distribution, we have essentially 2 goals (generally, in any ML problem):</p>

<ul>
  <li>Training</li>
  <li>Inference</li>
</ul>

<h2 id="training">Training</h2>

<p>On this issue, the first question one has in mind is, how can we reduce our probabilities from before (i.e. \(P(positive | text) = P(positive | word\_freq_1 , word\_freq_2, ...)\) ) to a combination of \(P(c)\) and \(P(f_i | c)\), since these last 2 represent how we actually define our generative model. The answer is, as the title of this post suggests, <strong>Bayes</strong>.</p>
<h3 id="bayes-rule">Bayes’ rule</h3>

<p>Simply put, we can decompose the probability using Bayes’ rule as follows:</p>

<p>\(P(positive | word\_freq_1 , word\_freq_2, ...) = \frac{P(word\_freq_1 , word\_freq_2, ... | positive) \cdot P(positive)}{P(word\_freq_1 , word\_freq_2, ...)}\)
<br />
Does this look familiar? If we have a look at the numerator, we see the similarity between those probabilities and the ones that define our generative model. We need some further processing on the first term - \(P(positive | word\_freq_1, ...)\) - in order to get exactly what we need.</p>

<h3 id="naïve-bayes">Naïve Bayes</h3>

<p>Let us have a look at the numerator from the Bayes’ rule corresponding equation.</p>

<p>We can identify - \(P(positive)\) - from that equation. 
\(P(positive | word\_freq_1 , ...)\) looks also similar, but not identical. What can we do to make it identical? The answer is the <em>Naïve Bayes assumption</em>. In a nutshell, we assume independence between the features given the class. To realize why it is naïve, let us walk through the following example. Assume we have the next corpus of data:</p>

<ul>
  <li>
    <p><em>In my opinion, the movie is not so bad.</em> - <span style="color:green">positive</span></p>
  </li>
  <li>
    <p><em>In my opinion, the movie is bad.</em> - <span style="color:red">negative</span></p>
  </li>
  <li>
    <p><em>In my opinion, the movie is quite bad.</em> - <span style="color:red">negative</span></p>
  </li>
</ul>

<p>If we consider the word <em>bad</em> and we compute the probability of its existence given the class, we would intuitively get: \(P(bad|positive) = \frac{1}{3}\).
However, if we consider the other features as well, we get a larger probability, i.e. \(P(bad|positive, not, so) = 1\), which is essentially 100% chance of having the word <em>bad</em> in our document, since we have only such an example.
So, as we can see, <em>bad</em> can have positive connotations given other features from the text.
That’s the main reason why the assumption that \(P(bad|positive, not, so) = P(bad|positive)\) is called <em>naïve</em>.</p>

<h2 id="inference">Inference</h2>

<p>Moving forward, we now have almost all the ingredients to predict the class of a certain document. Let us examine the Bayes’ rule equation to understand what is missing. After considering the naïve assumption, what we end up with is:
\(P(positive | word\_freq_1 , word\_freq_2, ...) = \frac{P(word\_freq_1 | positive) \cdot P( word\_freq_2|positive) \cdot ... \cdot P(positive)}{P(word\_freq_1 , word\_freq_2, ...)}\)</p>

<p>Essentially, at this point, assume we have learned the prior \(P(positive)\) and the probabilities of the features \(P(word\_freq|class)\) that characterize our generative model. What we still need in order to predict the class of the text is the denominator from the equation from above \(P(word\_freq1, word\_freq2, ...)\). Since this is not normally easy to compute (we have to consider all possible combinations of all the features), we can simply drop it and consider only the numerator for the classification of the text. The consequence is that we do not have a probability distribution anymore (since we drop the denominator, which acts as a normalizing term), but the proportionality still holds, thus giving us the more likely class with the higher score.</p>
<h2 id="training-1">Training</h2>

<p>Up until now, we have seen how to actually use our model to predict the class of a certain text. The way this normally works, as in any Machine Learning setup, we first obtain these probabilities via training and then try to perform inference. In order to obtain these probabilities, we need to get the parameters of the corresponding distributions, by using the training data in order to estimate them. For getting the right parameters, we can use various approaches, among which Maximum Likelihood Estimation, Maximum a Posteriori, or full Bayesian approaches by estimating the full posterior are all suitable choices.</p>]]></content><author><name></name></author><category term="naive-bayes" /><category term="basic-ml" /><summary type="html"><![CDATA[Requirements Deep understanding of Bayes’ rule Machine Learning fundamentals (Maximum Likelihood Estimation, what inference, training mean)]]></summary></entry><entry><title type="html">Lagrangian</title><link href="https://theostoican.github.io/blog/2022/lagrangian/" rel="alternate" type="text/html" title="Lagrangian" /><published>2022-05-28T00:00:00+00:00</published><updated>2022-05-28T00:00:00+00:00</updated><id>https://theostoican.github.io/blog/2022/lagrangian</id><content type="html" xml:base="https://theostoican.github.io/blog/2022/lagrangian/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>In machine learning, it is often the case that one ends up (after various modeling steps) with an optimization problem. For instance, in SVMs, one has to find the optimal hyperplane that maximizes the margin. Furthermore, optimization is generally potentially complicated due to constrains that one imposes on the optimal point that is to be found. According to the taxonomy of optimization problems, this type of problems is part of the class of <em>constrained optimization</em> problems. If the objective function that one wants to optimize (and typically minimize) is convex, there are two major techniques that are typically used in order to achieve this.</p>

<h1 id="the-lagrange-multipliers">The Lagrange multipliers</h1>

<p>Let us assume we have a convex objective function \(f(x)\) that we want to optimize (in particular, we will talk about minimization, since any maximization problem can be reduced to a minimization problem). Furthermore, there are a couple of additional constraints that we may want to impose on the minimum that we find. For now we will just focus on affine and equality constraints, since the method that we are going to investigate offers guarantees only for this class of constraints. To sum up, the problem that we’re looking at is of the form:</p>

\[\min_{x \in Dom(f)} f(x)\]

\[g_i(x) = 0, \forall i \in {1..n}\]

<p>, where \(f(x)\) - convex and \(g_i(x)\) - affine constraints.</p>

<h2 id="intuition">Intuition</h2>

<p>Let us assume we want to optimize \(f(x)\) without any constraints. To do this, one naturally looks at the gradient. We can just enforce \(\nabla f(x) = 0\) and we find the \(x\) at which this holds (assuming the function is convex). With the constraints, however, this idea does not work anymore, since the minimum of \(f(x)\) may not fulfil \(g_i(x) = 0\) for some \(i\). Geometrically, in order to visualize this, let us assume (without loss of generality) we have only one constraint.</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lagrangian/min_f_not_constrained_min-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lagrangian/min_f_not_constrained_min-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lagrangian/min_f_not_constrained_min-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lagrangian/min_f_not_constrained_min.png" class="img-fluid w-50 rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p align="center">
An example where the minimum of an objective function is not identical to the constrained minimum. Made with &copy; Desmos.
</p>
</div>

<p>As we can see in the picture from above, the constrained minimum does not coincide with the minimum of \(f\) and it is actually materialized at \(x_{min} = (-0.63, 0.38)\). This is a bit to the left of the minimum of \(f\). And, more importantly, \(\nabla f(x_{min})\) is not \(0\). To find this type of points analytically, Lagrange figured out that, instead of looking at where the gradient is 0, one should look instead at where the gradients of the constraint function and the objective function are parallel. This intuition is very revealing, due to the fact that the set of points among which we’re searching our minimum must be the set of points on the constraint line. If we take a certain point on a line and there exists another point that leads to a better minimum on the same line, then the gradient of \(f\) must point towards it. Therefore, it cannot be perpendicular on the tangent line to the curve at this point. Meanwhile, the gradient of the constraint function is always perpendicular on the line (one can also visualize it as the normal vector of the line). Similarly, if there is no other point better than the current point, then the gradient of \(f\) must be perpendicular on the tangent (there is no better minimum on the constraint line, but there is one potentially outside of it - however, we are not interested in it). But since this point is on the constraint line, the gradient of \(g\) (the constraint function) must also be perpendicular on the constraint line (and on the tangent, by extension). Therefore, by ensuring that the two gradients are parallel, we are sure that there cannot be a more optimal point than this. This can be visualized as follows:</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lagrangian/LagrangeMultipliers2D-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lagrangian/LagrangeMultipliers2D-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lagrangian/LagrangeMultipliers2D-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lagrangian/LagrangeMultipliers2D.png" class="img-fluid rounded z-depth-1 w-50" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

An example of Lagrange multipliers for 2D objective and constraints functions (&copy; https://en.wikipedia.org/wiki/Lagrange_multiplier).
</div>

<h2 id="formalism">Formalism</h2>

<p>Analytically, this parallelism can also be described with the following equation:</p>

\[\nabla f(x) = \lambda \nabla g(x)\]

\[g(x) = 0\]

<p>\(\lambda\) is a scalar that ensure that the vector gradients have the same direction, but not necessarily the same length. The second equation ensures that the point is on the constraint line. By solving this equation, one can find both the optimal \(x\) and \(\lambda\). This can also be naturally extended to multiple constraints \(g_i(x)\), \(\forall i \in {1..N}\).</p>

<h1 id="the-lagrangian">The Lagrangian</h1>

<p>Below we’ll provide a justification and an intuition for the formulation of the Lagrangian.</p>

<h2 id="justification">Justification</h2>

<p>Problems in practice can be even more complex than this. In particular, one can find optimization problems, for which the contraint functions can be inequalities. For these situations, an extension of the method would be required. To understand why, let us have a look at the following example. Assume we have an objective function \(f(x)=x^2\) with one constraint \(2 \cdot x \leq 1\) (without loss of generalization we treat these constraints; constraints of the type \(c(x) \geq a\) can be reformulated into \(c'(x) \leq -a\), where \(c'(x)=-c(x)\)), which can be rephrased as follows:</p>

\[f(x) = x^2\]

\[g(x) \leq 0\]

<p>In the equations, I have rewritten the constraint as \(g(x) = 2 \cdot x - 1\). Let us have a look at the graph of these functions.</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lagrangian/example_ineq_constraint-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lagrangian/example_ineq_constraint-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lagrangian/example_ineq_constraint-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lagrangian/example_ineq_constraint.png" class="img-fluid rounded w-50 z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

An example of optimization problem where the optimal solution is different for inequality constraints, compared to equality constraints. Made with &copy; Desmos.
</div>

<p>If one looks at the chart from above, one sees that the minimum of the problem with an inequality constraint does not coincide with the minimum for the problem with an equality constraint. By imposing \(g(x) = 0\), the method of Lagrange multipliers would find the minimum on the constraint line. However, that does not coincide with the real optimum, which is actually the same as the minimum of \(f(x)\). For this reason, an adaptation of the method of Lagrange multipliers is required in order to make it work for inequality constraints.</p>

<h2 id="intuition-1">Intuition</h2>

<p>One can also see the finding of an optimum under inequality constraints from a different angle. One could engineer a surrogate function that penalizes the points for which the inequality constraints do not hold. This can be mathematically translated as follows:</p>

\[L(x, \lambda) = f(x) + \lambda \cdot g(x)\]

<p>This function that we just defined is called the Lagrangian. It is essentially a reformulation of the method of Lagrange multipliers for equality constraints that I described above. Its mechanics can be best understood when we look at it for \(\lambda \gt 0\). \(g(x)\) is always positive for points that do not fulfill the constraint. Hence, \(\lambda \cdot g(x)\) will add a penalty to points outside of the constraint. Conversely, \(\lambda \cdot g(x)\) will be negative for points that fulfil the constraint, since \(g(x) \lt 0\). This leads to lowering the value of \(L(x, \lambda)\) for points that overfulfil the constraint (\(g(x) \lt \lt 0\)).</p>

<p>Now, to solve this problem, one has to find a \(\lambda\), which is big enough to penalize all the points that do not fulfill the constraint, but which also does not overly reward points that “overfit” on the constraint, i.e. points for which \(g(x) \lt \lt 0\).</p>

<h2 id="formalism-1">Formalism</h2>

<p>Analytically, this can be phrased as finding \(\min_{x} f(x) + \lambda g(x)\) for every possible \(\lambda\) (which can be formulated as \(g(\lambda) = \min_{x} f(x) + \lambda g(x)\)) and then optimizing over \(\lambda\), by finding \(\max_{\lambda} g(\lambda)\). Optimizing \(g\) is called, in literature, the dual problem. So, in a nutshell, finding the minimum under the constraints can be rephrased as:</p>

\[\max_{\lambda} \min_{x} L(x, \lambda)\]

<p>This also means finding the saddle point of the Lagrangian. To see why this works, let us look at an example. To keep things simple, we will look at an optimization problem with one inequality constraint only (without loss of generality):</p>

\[\min_{x} f(x)\]

\[g(x) = x+2 \lt 0\]

<p>The chart of this function would be:</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lagrangian/lagrangian_simple_problem-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lagrangian/lagrangian_simple_problem-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lagrangian/lagrangian_simple_problem-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lagrangian/lagrangian_simple_problem.png" class="img-fluid w-50 rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

An example of optimization problem where we have a convex objective and one inequality constraint. Made with &copy; Desmos.
</div>

<p>The optimum that we’re interested in is at \(x = -2\). Now, let us look at the dual function for certain values of \(\lambda\).</p>

<div style="text-align: center;">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lagrangian/lagrangian_multiple_lambda-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lagrangian/lagrangian_multiple_lambda-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lagrangian/lagrangian_multiple_lambda-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lagrangian/lagrangian_multiple_lambda.png" class="img-fluid w-50 rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

The same problem from before, where we see the dual for multiple values of lambda. Made with &copy; Desmos.
</div>

<p>As one can see, the more we increase \(\lambda\), the more the unconstrained minimum of \(f\) (at \(x=0\)) is penalized. That is the reason why the constrained minimum (\(g(\lambda)\)) has to move to the left of the unconstrained minimum, so that it is closer to the constraint line. At the same time, the more we move to the left (up to a certain point as we will see), the larger the value of the constrained minimum (than the previous constrained minimum) is. This is due to the fact that we are in a regime where the first terms of \(g\) is dominant (\(f(x)\)) and moving even slightly to the left entails moving farther from the unconstrained minimum of \(f\) and thus having higher values for \(f(x)\).</p>

<p>However, by going left farther and farther and entering the area where the constraint is fulfilled (see \(\lambda=5\) in the figure from above), we enter a regime where the second term of \(g\) is dominant (\(\lambda g(x)\)). This term is negative, since the constraint is fulfilled. By increasing \(\lambda\), we only increase the influence of this term and thus generating smaller and smaller constrained minima (up to \(-\inf\)), that are far from what we are interested in.</p>

<p>The point we are actually interested in is at the intersection of the 2 regimes. \(\lambda\) should be high enough such that all points to the right of the constraint line are sufficiently penalized and can not be optima anymore (the first term of \(g\) is not overly influential), but small enough such that the points that are well into the constrained area are not overly appreciated (the second term of \(g\) is not overly powerful). This point in which we are interested corresponds to \(\lambda=4\) in the figure from above. It also corresponds to the point for which \(g(x)=0\) (exactly on the edge of the constrained area). Finding this point corresponds to finding the maximum of \(g\) and finding the maximum of \(g\) entails solving $$\max_{\lambda}\min_{x} L(x, \lambda), thus the saddle point of the Lagrangian.</p>

<p>This method, like the one of the Lagrange multipliers for equality constraints, can naturally be generalized to multiple constraints.</p>

<h2 id="guarantees">Guarantees</h2>

<p>Unfortunatelly, this algorithm does not necessarily provide theoretical guarantees for all class of problems. For instance, if the problem is not convex, then solving the dual will find a point that will be at a certain gap than the real optimum. The KKT conditions provide theoretical gurantees for the optima that are found, by defining (what is called in literature) <em>strong duality</em> (no gap between the optimum that is found and the real optimum) and the conditions to verify it for a specific problem.</p>

<h1 id="references">References</h1>

<p>[1] https://cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf</p>

<p>[2] https://masszhou.github.io/2016/09/10/Lagrange-Duality/</p>

<p>[3] https://math.stackexchange.com/questions/223235/please-explain-the-intuition-behind-the-dual-problem-in-optimization</p>]]></content><author><name></name></author><category term="Lagrangian" /><category term="basic-ml" /><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>